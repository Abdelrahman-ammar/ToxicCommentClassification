{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "import datetime\n",
    "import tensorboard\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model \n",
    "from tensorflow.keras.layers import LSTM , Dense ,Dropout , Input , Embedding , Bidirectional\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud , STOPWORDS\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping , TensorBoard\n",
    "from tensorflow.keras.models import load_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "df = pd.read_csv(\"Train/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77591</th>\n",
       "      <td>cfe1130a315bc5e1</td>\n",
       "      <td>Knife in the back\\n I came to you for help 3 times now... Is that something the bischo clan of puppets do? I think NOT! I actually started to like you and thought you were a nice guy sharing the music info but you know how to fool people. The knife i picked up (as you suggested in the investigation article) is the one you stuck in my back! I thought that maybe i found a decent person in you... \\nThis community in wikipedia needs to learn to work together... and not always against everyone!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40370</th>\n",
       "      <td>6bb86d75cfd9a108</td>\n",
       "      <td>Thankyou. I honestly think that you should soon become an administrator. I would nominate you if i could and if you would like. BTW I think you should archive your  talk page pretty soon:)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id  \\\n",
       "77591  cfe1130a315bc5e1   \n",
       "40370  6bb86d75cfd9a108   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         comment_text  \\\n",
       "77591  Knife in the back\\n I came to you for help 3 times now... Is that something the bischo clan of puppets do? I think NOT! I actually started to like you and thought you were a nice guy sharing the music info but you know how to fool people. The knife i picked up (as you suggested in the investigation article) is the one you stuck in my back! I thought that maybe i found a decent person in you... \\nThis community in wikipedia needs to learn to work together... and not always against everyone!   \n",
       "40370                                                                                                                                                                                                                                                                                                                    Thankyou. I honestly think that you should soon become an administrator. I would nominate you if i could and if you would like. BTW I think you should archive your  talk page pretty soon:)   \n",
       "\n",
       "       toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "77591      0             0        0       0       0              0  \n",
       "40370      0             0        0       0       0              0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words  = set(stopwords.words(\"english\"))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_patterns = [r\"a+w+\" , r\"w+p+\" , r\"u+h+\" , r\"w+\" , r\"im\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_words = {\n",
    "    \"fuck\": [\"fucking\" , \"fuckin\" , \"f*$%-ing\"] ,\n",
    "    \"dick\" : [\"dihck\"],\n",
    "    \"nigga\" : [\"nihgaa\", \"nigger\"],\n",
    "    \"ass\" : [\"arse\"] \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    sentence = re.sub(r\"\\n\",\" \",sentence)\n",
    "    sentence = re.sub(r\"[( )]\",\" \",sentence)\n",
    "    \n",
    "    words = [word for word in sentence.split() if len(word)> 1]\n",
    "    \n",
    "    for i in range(len(words)):\n",
    "        \n",
    "        words[i] = wnl.lemmatize(words[i])\n",
    "    sentence = \" \".join(words)\n",
    "    \n",
    "    sentence = decontracted(sentence)\n",
    "\n",
    "    for pattern in custom_patterns:\n",
    "        sentence = re.sub(pattern, '', sentence)\n",
    "\n",
    "    for word,variant in custom_words.items():\n",
    "        for var in variant:\n",
    "            sentence = str(sentence).replace(var,word)\n",
    "            \n",
    "\n",
    "    sentence = re.sub(r\"[^a-zA-Z]\",\" \",sentence)\n",
    "    \n",
    "    sentence = re.sub(\"([^\\x00-\\x7F])+\",\" \",sentence)\n",
    "\n",
    "    sentence = re.sub(r\"\\s+\",\" \",sentence)\n",
    "    \n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    # words = sentence.split()\n",
    "    \n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = [word for word in phrase.split() if word not in stop_words]\n",
    "    \n",
    "    phrase = \" \".join(phrase)\n",
    "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"weren\\'t\" , \"were not\",phrase)\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    \n",
    "    return phrase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gay nigga god damndd gay nigga fuck fuck shtay ay kcik talk page bitch angry baby plz po anymore atalk dicuser page going cum ur house kill faggity\n"
     ]
    }
   ],
   "source": [
    "print(clean_sentence(df[df[\"obscene\"]==1][\"comment_text\"][141267]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"id\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()\n",
    "df_copy[\"comment_text\"] = df_copy[\"comment_text\"].apply(lambda x : clean_sentence(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2157                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        thank vigilant mission prevent people laughing ant anyone forget serious place portant are glumness constipation remain faithful editor\n",
       "40963                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         hi need are editing rong articles article eastern cicada killer already ha link coelho is holliday is page trying insert properly placed external link section bottom link go top article please edit cicada killer asp article redirect sphecius article redirects to one specie genus one specie familiar ith citation regarding specie already included ikipedia ould fine noted information trying add already there already correct place thanks\n",
       "82392                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              qing dynasty qing korean attack korea make pay tribute force hermit kingdom force erect humiliating stelea honor\n",
       "132845                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     mentioned earlier support move understand mark marathon is concern appreciate clarification he care enough oppose move fao peer revieed journals regular user google scholar resources including past contribution iki is sorghum staple food articles fao articles southeast asia africa multiple languages author use sorghum english language title summary mean commercial crop training manual seet sorghum catalogue passport characterization data sorghum pearl millet finger millet germplasm zbabe cultural practice maize sorghum millet sorghum millet human nutrition fao source support mark is comment sorghum crop knon many names etc chapter authors nevertheless use sorghum names title various chapter mean commercial crop sorghum\n",
       "90968     condition problematic evidenced current ani discussion penalty violation steep community generally speaking going reluctant apply anything less clear unequivocal violation need remedy short indef gray area violations suggest folloing modifications starting point discussion nothing else event fry appears beginning encroach upon spirit civility ban uninvolved administrator may topic ban three day specific article article talk page becoming problematic notification ban made fry is talk page article talk page existing communication must refrain commenting individual editor except appropriate behavioral noticeboard pages understanding civil applies page also bad contradicts board is policies require discussing editor board first place fry untenable situation take until unless behavior editor is problematic there is chance ani boomerang changed something like all communication must refrain commenting individual editor except users talk page uninvolved administrator is talk pages appropriate behavioral noticeboard pages understanding civil applies page also uncool bringing individual editor is stuff article talk also add something like editor is concerned fry may danger violating unblock condition raise issue talk page article talk pages ne ent\n",
       "Name: comment_text, dtype: object"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_copy[\"comment_text\"].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glove Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult','identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_copy[\"comment_text\"].values\n",
    "y = df_copy[target_cols].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[512,\n",
       " 60,\n",
       " 68,\n",
       " 569,\n",
       " 4042,\n",
       " 10216,\n",
       " 706,\n",
       " 250,\n",
       " 16409,\n",
       " 5832,\n",
       " 2366,\n",
       " 2659,\n",
       " 52,\n",
       " 1076,\n",
       " 11541,\n",
       " 2508,\n",
       " 11,\n",
       " 160,\n",
       " 209,\n",
       " 7,\n",
       " 4,\n",
       " 80,\n",
       " 6,\n",
       " 19,\n",
       " 3111,\n",
       " 81]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_sequences = tokenizer.texts_to_sequences(X)\n",
    "x_sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_padded_sequences = pad_sequences(x_sequences,maxlen=200,padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_length = len(tokenizer.word_index )+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_file = 'glove.6B.300d.txt'\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]  \n",
    "        coefs = np.asarray(values[1:], dtype='float32')  \n",
    "        embeddings_index[word] = coefs  \n",
    "\n",
    "print(f\"Found {len(embeddings_index)} word vectors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros((vocab_length, 300))\n",
    "\n",
    "for word,i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    # print(embedding_vector)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 200)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_padded_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Lstm(x_train,y_train,x_val,y_val,epochs,batch_size,vocab_size):\n",
    "\n",
    "    input_layer = Input(shape=(200,),name=\"input layer\")\n",
    "\n",
    "    embed_layer = Embedding(vocab_size,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False,\n",
    "                            output_dim=300,\n",
    "                            name=\"embeddings\")\n",
    "    \n",
    "    embedded_seq = embed_layer(input_layer)\n",
    "\n",
    "    x = Bidirectional(LSTM(units=64,return_sequences=True,activation=\"tanh\" , name=\"lstm_1\"))(embedded_seq)\n",
    "\n",
    "    x = Dropout(rate=0.3 ,  name=\"dropout_layer\")(x)\n",
    "\n",
    "    x = Bidirectional(LSTM(units=32,return_sequences=False,activation=\"tanh\",name=\"lstm_2\"))(x)\n",
    "\n",
    "    output_layer = Dense(units=6 , activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=input_layer,outputs=output_layer)\n",
    "\n",
    "    model.compile(loss=\"binary_crossentropy\" , optimizer=\"adam\",metrics = [\"accuracy\"])\n",
    "\n",
    "\n",
    "    \n",
    "    model.fit(x_train,y_train,\n",
    "              validation_data=(x_val,y_val),\n",
    "              epochs=epochs,\n",
    "              batch_size= batch_size\n",
    "              )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train , x_test , y_train , y_test = train_test_split(x_padded_sequences,y , test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2 , x_val , y_train2 , y_val = train_test_split(x_train,y_train , test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m3192/3192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m906s\u001b[0m 282ms/step - accuracy: 0.9460 - loss: 0.0964 - val_accuracy: 0.9939 - val_loss: 0.0571\n",
      "Epoch 2/3\n",
      "\u001b[1m3192/3192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m922s\u001b[0m 289ms/step - accuracy: 0.9938 - loss: 0.0530 - val_accuracy: 0.9937 - val_loss: 0.0520\n",
      "Epoch 3/3\n",
      "\u001b[1m3192/3192\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1008s\u001b[0m 316ms/step - accuracy: 0.9910 - loss: 0.0468 - val_accuracy: 0.9938 - val_loss: 0.0503\n"
     ]
    }
   ],
   "source": [
    "model = build_Lstm(x_train2,\n",
    "                   y_train2,\n",
    "                   x_val,\n",
    "                   y_val,\n",
    "                   epochs=3,\n",
    "                   batch_size=32,\n",
    "                   vocab_size=vocab_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m998/998\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 126ms/step - accuracy: 0.9939 - loss: 0.0473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.048964209854602814, 0.9937019944190979]"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m998/998\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 103ms/step\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.29116   , 0.00138616, 0.01526344, 0.00879377, 0.0567487 ,\n",
       "       0.01017578], dtype=float32)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (predictions > (0.5)).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fuck bitch'"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"you fucking bitch\"\n",
    "\n",
    "text = clean_sentence(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[32, 514]]"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sequence = tokenizer.texts_to_sequences(np.array([text]))\n",
    "test_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n"
     ]
    }
   ],
   "source": [
    "padded_test_sequence = pad_sequences(test_sequence,200,padding=\"post\")\n",
    "test_predictions = model.predict(padded_test_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 1, 0, 1, 0]])"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(test_predictions > (0.5)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model , Tokenizer & Embedding Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"ToxicModelV2.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"tokenizer.pkl\",\"wb\") as file :\n",
    "    pickle.dump(tokenizer,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"embedding_matrix.pkl\",\"wb\") as file :\n",
    "    pickle.dump(embedding_matrix,file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
